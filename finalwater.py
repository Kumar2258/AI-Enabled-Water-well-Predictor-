# -*- coding: utf-8 -*-
"""finalwater.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nKAbZUpt4VjkvzF0cGqmyQKtenIOH6XU
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.metrics import accuracy_score

hyd_for=pd.read_csv("/content/newRajstanHydro.csv")
df=pd.read_csv("/content/NewRajstanChannged.csv")



label_encoder = preprocessing.LabelEncoder()
hyd_for['Hyd_Formation']= label_encoder.fit_transform(hyd_for['Hyd_Formation'])
df['Hyd_Formation']= label_encoder.fit_transform(df['Hyd_Formation'])

df.shape
hyd_for.describe()
df.describe()


print(df['Latitude'].value_counts())


x=df[["Latitude","Longitude","Hyd_Formation"]]
y=df[["Wl_post_2021_m_bbgl"]]

x_hyd=hyd_for[["Latitude","Longitude"]]
y_hyd=hyd_for["Hyd_Formation"]

xx=hyd_for[["Longitude"]]

yy=df[["Wl_post_2021_m_bbgl"]]

plt.scatter(xx,y_hyd)
plt.xlabel('a')
plt.ylabel('b')
plt.title('Plot of b against a')


plt.show()

xx=hyd_for[["Latitude"]]


yy=df[["Wl_post_2021_m_bbgl"]]

plt.scatter(xx,y_hyd)
plt.xlabel('a')
plt.ylabel('b')
plt.title('Plot of b against a')


plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
x_hyd_train, x_hyd_test, y_hyd_train, y_hyd_test = train_test_split(x_hyd, y_hyd, test_size=0.2, random_state=32)

# Define the parameter grid
# param_grid = {
#     'C': [0.1, 1, 10],          # Regularization parameter
#     'gamma': ['scale', 'auto'], # Kernel coefficient
#     'kernel': ['linear', 'rbf'] # Kernel type
# }

# Instantiate the SVC model
model_svc = SVC()

# Instantiate GridSearchCV with accuracy as the scoring metric
# grid_search = GridSearchCV(estimator=model_svc, param_grid=param_grid, cv=5, scoring='accuracy')

# Fit the GridSearchCV to the training data to find the best parameters
model_svc.fit(x_hyd_train, y_hyd_train)

# Get the best hyperparameters
# best_hyperparameters = grid_search.best_params_

# Print the best hyperparameters
# print("Best Hyperparameters:", best_hyperparameters)

# Get the best estimator (model) from the grid search
# best_model_svc = grid_search.best_estimator_

# # Predict the labels using the best model
y_pred = model_svc.predict(x_hyd_test)

# # Calculate the accuracy score
accuracy = accuracy_score(y_hyd_test, y_pred)

# # Print the accuracy score
print("Accuracy:", accuracy)

# HudrogeologicaL Model
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
x_hyd_train, x_hyd_test, y_hyd_train, y_hyd_test = train_test_split(x_hyd, y_hyd, test_size=0.2, random_state=32)
model_hyd=KNeighborsClassifier(algorithm='auto',n_neighbors=9,weights='distance')
model_hyd.fit(x_hyd_train, y_hyd_train)
hyd_val=np.array([[26.6832448,75.2335695]])
hyd_pred=int(model_hyd.predict(hyd_val))
y_hyd_pred=model_hyd.predict(x_hyd_test)
print(hyd_pred)
accuracy = accuracy_score(y_hyd_test, y_hyd_pred)
print("Accuracy-> ",accuracy)
# print(y_test)






param_grid = {
    'n_neighbors': [3, 5, 7, 9],  # Number of neighbors to consider
    'weights': ['uniform', 'distance'],  # Weight function used in prediction
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']  # Algorithm used to compute the nearest neighbors
}

# Instantiate the KNeighborsClassifier model
model_hyd = KNeighborsClassifier(algorithm='auto',n_neighbors=9,weights='distance')

# Instantiate GridSearchCV
grid_search = GridSearchCV(estimator=model_hyd, param_grid=param_grid, cv=5, scoring='accuracy')

# Fit the GridSearchCV to the training data to find the best parameters
grid_search.fit(x_hyd_train, y_hyd_train)

# Print the best hyperparameters
print("Best Hyperparameters:", grid_search.best_params_)

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_classification


x_hyd_train, x_hyd_test, y_hyd_train, y_hyd_test = train_test_split(x_hyd, y_hyd, test_size=0.2, random_state=32)
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=46)
# Split data into train and test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid to search
# param_grid = {
#     'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
#     'C': [0.1, 1, 10],
#     'gamma': ['scale', 'auto']
# }

# # Create the GridSearchCV object
# grid_search = GridSearchCV(estimator=SVC(), param_grid=param_grid, cv=5, scoring='accuracy')
model_hyd= SVC(C=0.1, gamma='scale', kernel='linear')
# Fit the GridSearchCV object to find the best parameters
model_hyd.fit(x_hyd_train, y_hyd_train)
y_pred=model_hyd.predict(x_hyd_test)
accuracy=accuracy_score(y_pred,y_hyd_test)
print(accuracy,"accuracy",)
hyd_val=np.array([[26.6832448,75.2335695]])
hyd_pred=int(model_hyd.predict(hyd_val))

# Get the best parameters and best score
# best_params = grid_search.best_params_
# best_score = grid_search.best_score_

# print("Best Parameters:", best_params)
# print("Best Score:", best_score)

#Model for Water level
from sklearn.ensemble import GradientBoostingRegressor

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=46)
model=GradientBoostingRegressor()
model.fit(x_train,y_train)
uservalue=np.array([[26.578406,75.5684254,hyd_pred]])
pred=model.predict(uservalue)
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_regression
print(int(pred))

from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_regression

# param_grid = {
#     'n_estimators': [70, 200, 300],
#     'learning_rate': [0.01, 0.2, 0.3],
#     'max_depth': [1, 5, 15]
# }
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=46)
# grid_search = GridSearchCV(estimator=GradientBoostingRegressor(), param_grid=param_grid, cv=5, scoring='r2')

# Fit the GridSearchCV object to find the best parameters

from sklearn.ensemble import RandomForestRegressor

# Instantiate RandomForestRegressor with best hyperparameters
model = RandomForestRegressor(max_features='sqrt', n_estimators=100)
# model=GradientBoostingRegressor(learning_rate=0.1, max_depth=3, n_estimators=300)
model.fit(x_train, y_train)
y_pred=model.predict(x_test)
from sklearn.metrics import r2_score
r2score=r2_score(y_test,y_pred)
print("R2 Score -> ",r2score)

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

# Assuming you have already trained your model and defined y_train_pred and y_test_pred
# mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_pred)

# print("Mean Squared Error (Training):", mse_train)
print("Mean Squared Error (Test):", mse_test)




# Assuming you have already trained your model and defined y_train_pred and y_test_pred
# mae_train = mean_absolute_error(y_train, y_train_pred)
mae_test = mean_absolute_error(y_test, y_pred)

# print("Mean Absolute Error (Training):", mae_train)
print("Mean Absolute Error (Test):", mae_test)
# grid_search.fit(x_train, y_train)

# # Get the best parameters and best score
# best_params = grid_search.best_params_
# best_score = grid_search.best_score_

# print("Best Parameters:", best_params)
# print("Best Score:", best_score)


# model=GradientBoostingRegressor()
# model.fit(x_train,y_train)
# uservalue=np.array([[26.578406,75.5684254,hyd_pred]])
# pred=model.predict(uservalue)
# print(int(pred))

plt.show()
X_new = np.linspace(0, 103, 103).reshape(-1, 1)
print(y_pred.shape)
# Plot the scatter graph for predicted values (red color)
plt.scatter(X_new, y_pred, color='red', label='Predicted')
plt.scatter(X_new,y_test,color='blue',label='Actual')
# Add labels and title
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Scatter Plot of Actual vs Predicted Values')

# Add a legend
plt.legend()

# Show the plot
plt.show()

# plt.scatter(range(0,200), y_test, color='blue', label='Actual')
plt.show()
X_new = np.linspace(0, 113, 113).reshape(-1, 1)
print(y_pred.shape)
# Plot the scatter graph for predicted values (red color)
plt.scatter(X_new, y_pred, color='red', label='Predicted')
plt.scatter(X_new,y_test,color='blue',label='Actual')
# Add labels and title
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Scatter Plot of Actual vs Predicted Values')

# Add a legend
plt.legend()

# Show the plot
plt.show()

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_classification

# x, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=46)
# Split data into train and test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid to search
param_grid = {
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
    'C': [0.1, 1, 10],
    'gamma': ['scale', 'auto']
}

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=SVC(), param_grid=param_grid, cv=5, scoring='accuracy')


model = SVC(C=0.1, gamma='scale', kernel='linear')

# Fit the GridSearchCV object to find the best parameters
model.fit(x_train, y_train)
y_pred=model.predict(x_test)
grid_search.fit(x_train, y_train)
mae_test = mean_absolute_error(y_test, y_pred)
# accuracy=accuracy_score(x_test,y_pred)
print(mae_test,"mean")
# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Score:", best_score)

print(y_train)

plt.scatter(range(0,200), y_test, color='blue', label='Actual')
plt.show()
# Plot the scatter graph for predicted values (red color)
plt.scatter(y_test, y_pred, color='red', label='Predicted')

# Add labels and title
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Scatter Plot of Actual vs Predicted Values')

# Add a legend
plt.legend()

# Show the plot
plt.show()

from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# Create sample data (replace X, y with your data)
# x, y = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=42)

# Split data into train and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=46)

# Define the parameter grid to search
param_grid = {
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
    'C': [0.1, 1, 10],
    'gamma': ['scale', 'auto']
}

# Create the GridSearchCV object
grid_search = GridSearchCV(estimator=SVR(), param_grid=param_grid, cv=5, scoring='r2')

# Fit the GridSearchCV object to find the best parameters
grid_search.fit(x_train, y_train)

# Get the best parameters and best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Score:", best_score)

from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor
x_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
model=RandomForestRegressor(max_features= 'auto', n_estimators= 200)
model.fit(x_train,y_train)
y_pred=model.predict(x_test)
r2 = r2_score(y_test, y_pred)

print("R2 Score:", r2)
print(y_train.shape)

# X_new = np.linspace(0, 10, 200).reshape(-1, 1)

# Predict output values for the new input data


# Plot the predicted values
plt.figure(figsize=(10, 6))
plt.scatter(x_train, y_train, color='blue', label='Training Data')
plt.plot(x_test, y_pred, color='red', linewidth=2, label='Regression Line')
plt.title('Linear Regression')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet

# Create sample data (replace X, y with your data)
# x, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)

# Split data into train and test sets
x_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Define models and their respective parameter grids for hyperparameter tuning
models = {
    # 'GradientBoostingRegressor': (GradientBoostingRegressor(), {'n_estimators': [100, 200, 300], 'learning_rate': [0.01, 0.1, 0.5], 'max_depth': [3, 5, 7]}),
    'RandomForestRegressor': (RandomForestRegressor(), {'n_estimators': [100, 200, 300], 'max_features': ['auto', 'sqrt', 'log2']}),
    'AdaBoostRegressor': (AdaBoostRegressor(), {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}),
    'KNeighborsRegressor': (KNeighborsRegressor(), {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}),
    #'Ridge': (Ridge(), {'alpha': [0.01, 0.1, 1, 10]}),
    #'Lasso': (Lasso(), {'alpha': [0.01, 0.1, 1, 10]}),
    #'ElasticNet': (ElasticNet(), {'alpha': [0.01, 0.1, 1, 10], 'l1_ratio': [0.1, 0.5, 0.9]})
}
from sklearn.metrics import r2_score
best_model = None
best_score = float('-inf')
# model= Ridge(alpha=0.01)
# model.fit(x_train,y_train)



# Assuming you have trained your model and made predictions
# y_pred = model.predict(x_test)

# # Calculate R2 score
# r2 = r2_score(y_test, y_pred)

# print("R2 Score:", r2)

# Iterate over models and find the best one
for name, (model, param_grid) in models.items():
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
    grid_search.fit(x_train, y_train)
    score = grid_search.best_score_
    if score > best_score:
        best_score = score
        best_model = (name, grid_search.best_estimator_, grid_search.best_params_)

print("Best Model:", best_model[0])
print("Best Hyperparameters:", best_model[2])
print("Best Mean Squared Error:", -best_score)

#Code for Drilling methods


drill_type = {
      0:"Consist of hard, crystalline rocks --> Rotary drilling with diamond or tungsten carbide bits or Down-the-hole hammer drilling",
      1:"Consist of hard, crystalline rocks --> Rotary drilling with diamond or tungsten carbide bits or Down-the-hole hammer drilling",
      2:"Consist of sand, gravel, and silt --> Auger drilling or Percussion drilling",
      3:"Consist of hard and abrasive rocks --> Rotary drilling with diamond or tungsten carbide bits or Down-the-hole hammer drilling",
      4:"Consist of hard and abrasive rocks --> Rotary drilling with diamond or tungsten carbide bits or Down-the-hole hammer drilling",
      5:"Consist Variation in Hardness -->Rotary drilling with suitable bits or down-the-hole hammer drilling",
      6:"Consist of sand, gravel, and silt --> Auger drilling or Percussion drilling",
      7:"Consist of sand, gravel, and silt --> Auger drilling or Percussion drilling",


}
for hyd_formation, drilling in drill_type.items():
  if(hyd_formation==hyd_pred):
      print(drilling)

  else :
      continue

if(0<pred<30):
  print("Excellent for Constuction")
elif(31 <pred<60):
  print("Good for Construction")
elif(61<pred<100):
  print("Fair for Construction")
else:
  print("Poor for Construction")



pur_data=pd.read_excel("/content/sample_data/purity.xlsx")

pur_data.head()

x=pur_data[['Latitude ','Longitude']]
y=pur_data[["pH"]]

x_p_train, x_p_test, y_p_train, y_p_test = train_test_split(x, y, test_size=0.2, )
modelp=GradientBoostingRegressor()
from sklearn.ensemble import RandomForestRegressor
model2=RandomForestRegressor()

model2.fit(x_p_train,y_p_train)
# uservalue=np.array([[26.578406,75.5684254,hyd_pred]])
predp=model2.predict(x_p_test)
# print(int(predp))

from sklearn.metrics import r2_score
r2 = r2_score(y_p_test, predp)
print(r2)

